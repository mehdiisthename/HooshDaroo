<div align="center">

<img src="logo/hooshdaroo.png" alt="HooshDaroo Logo" width="180"/>

# üíä HooshDaroo (ŸáŸàÿ¥ ÿØÿßÿ±Ÿà)

### AI-Powered Persian Pharmaceutical Assistant

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.x-red?logo=streamlit)](https://streamlit.io/)
[![Neo4j](https://img.shields.io/badge/Neo4j-Graph%20DB-brightgreen?logo=neo4j)](https://neo4j.com/)
[![FAISS](https://img.shields.io/badge/FAISS-Vector%20DB-orange)](https://github.com/facebookresearch/faiss)
[![Ollama](https://img.shields.io/badge/Ollama-Gemma3%3A4b-lightgrey?logo=ollama)](https://ollama.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

**HooshDaroo** is a bilingual (Persian/English) pharmaceutical chatbot that combines Retrieval-Augmented Generation (RAG) and Graph RAG over a pharmaceutical knowledge graph to deliver accurate, traceable, and evidence-backed answers to drug-related questions ‚Äî powered entirely by local LLMs.

> üèÜ Developed as the final capstone project for the **Hamrahe Aval (MCI) AI Bootcamp** ‚Äî February 2026.

</div>

---

## ‚ú® Features

- üîç **Hybrid Retrieval** ‚Äî Combines BM25 sparse search and FAISS dense semantic search for robust context retrieval
- üï∏Ô∏è **Graph RAG** ‚Äî Queries a Neo4j pharmaceutical knowledge graph using LLM-extracted entities and structured Cypher queries
- üß† **LLM-Based Entity Extraction** ‚Äî Recognizes drugs, conditions, adverse effects, drug classes, populations, and more from user queries
- üåê **Bilingual Support** ‚Äî Handles questions in both Persian (Farsi) and English; always responds in fluent Persian
- üí¨ **Multi-Session Chat Management** ‚Äî Create, name, and switch between multiple conversations with automatic summarization
- üìñ **Evidence-Backed Answers** ‚Äî Every answer is grounded in retrieved context from the knowledge base; no hallucinated facts
- üèÉ **Fully Local Inference** ‚Äî Answer generation via [Ollama](https://ollama.com/) + `gemma3:4b`, no external LLM API calls needed for generation
- üìä **LLM-as-a-Judge Evaluation** ‚Äî Built-in evaluation pipeline to benchmark answer quality
- üé® **Custom Dark UI** ‚Äî Clean, responsive Streamlit interface with RTL Persian text support

---

## üèóÔ∏è System Architecture

HooshDaroo uses a **dual-retrieval pipeline** that runs both a vector-based RAG and a knowledge-graph-based GraphRAG in parallel, fusing their contexts before sending to the generator LLM.

```
User Query (Persian/English)
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Query Analysis Layer                        ‚îÇ
‚îÇ   Entity Extractor (AvalAI LLM)  ‚îÇ  Persian Text Normalizer (hazm)‚îÇ
‚îÇ   Entities: Drug, Condition,     ‚îÇ  Intent Classification:        ‚îÇ
‚îÇ   AdverseEffect, DrugClass, ...  ‚îÇ  INDICATION, INTERACTION, ...  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ                           ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ     RAG Pipeline   ‚îÇ    ‚îÇ   Graph RAG Pipeline   ‚îÇ
          ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ    ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
          ‚îÇ  BM25 (sparse)   ‚îÇ    ‚îÇ  Entity Linking         ‚îÇ
          ‚îÇ       +          ‚îÇ    ‚îÇ  (exact + fuzzy match)  ‚îÇ
          ‚îÇ  FAISS (dense)   ‚îÇ    ‚îÇ  ‚Üì                      ‚îÇ
          ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ    ‚îÇ  Neo4j Cypher Queries   ‚îÇ
          ‚îÇ  Hybrid Fusion   ‚îÇ    ‚îÇ  (Indication, Interaction‚îÇ
          ‚îÇ  (alpha=0.5)     ‚îÇ    ‚îÇ   AdverseEffect, ...)   ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ                           ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ     Generation Layer         ‚îÇ
                    ‚îÇ   Ollama + gemma3:4b (local) ‚îÇ
                    ‚îÇ   Streaming response         ‚îÇ
                    ‚îÇ   Persian output + sources   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Project Structure

```
HooshDaroo/
‚îú‚îÄ‚îÄ app.py                  # Main Streamlit web application
‚îú‚îÄ‚îÄ answer.py               # Orchestrates RAG + GraphRAG + LLM generation
‚îú‚îÄ‚îÄ rag.py                  # Hybrid BM25 + FAISS retrieval pipeline
‚îú‚îÄ‚îÄ graphrag.py             # Full Graph RAG pipeline (entity linking, Cypher queries)
‚îú‚îÄ‚îÄ entity_extractor.py     # LLM-based NER & intent classification
‚îú‚îÄ‚îÄ make_vector_db.py       # Script to build the Milvus vector database
‚îú‚îÄ‚îÄ chat.py                 # Multi-session chat management & summarization
‚îú‚îÄ‚îÄ prompts.py              # System and summarization prompts
‚îú‚îÄ‚îÄ evaluation.py           # LLM-as-a-Judge evaluation pipeline
‚îú‚îÄ‚îÄ utils.py                # Shared utilities (token counting, ID generation)
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ file.env                # Environment variable template (rename to .env)
‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îî‚îÄ‚îÄ styles.css          # Custom Streamlit styles
‚îú‚îÄ‚îÄ logo/
‚îÇ   ‚îî‚îÄ‚îÄ hooshdaroo.png      # Application logo
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ rag_chunks_v5.json  # Preprocessed pharmaceutical document chunks
‚îú‚îÄ‚îÄ chats/                  # Persisted chat sessions (auto-created)
‚îú‚îÄ‚îÄ graph_refs/             # Saved GraphRAG query results (auto-created)
‚îî‚îÄ‚îÄ test/
    ‚îú‚îÄ‚îÄ questions.json       # Evaluation question set
    ‚îî‚îÄ‚îÄ results_1.json       # Evaluation results output
```

---

## ‚öôÔ∏è Tech Stack

| Component | Technology |
|---|---|
| **Frontend** | Streamlit |
| **Vector Database** | FAISS (runtime) / Milvus (indexing) |
| **Knowledge Graph** | Neo4j |
| **Embedding Model** | `QWEN3-Embedding-0.6B` (via SentenceTransformers) |
| **Local LLM (Generation)** | `gemma3:4b` via Ollama |
| **LLM API (Entity Extraction)** | AvalAI (OpenAI-compatible) |
| **Sparse Retrieval** | BM25 via `rank_bm25` |
| **Persian NLP** | `hazm` (normalization, tokenization) |
| **Fuzzy Matching** | `rapidfuzz` |
| **Token Counting** | `tiktoken` (`cl100k_base`) |

---

## üöÄ Getting Started

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.com/) installed and running
- [Neo4j](https://neo4j.com/download/) instance (local or cloud)
- A GPU is recommended for embedding inference (CUDA)

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/HooshDaroo.git
cd HooshDaroo
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Pull the Local LLM

```bash
ollama pull gemma3:4b
```

### 4. Configure Environment Variables

Copy the template and fill in your settings:

```bash
cp file.env .env
```

Then edit `.env`:

| Variable | Description | Default |
|---|---|---|
| `EMBEDDING_MODEL` | Path or HuggingFace name for the embedding model | `QWEN/QWEN3-Embedding-0.6B` |
| `EMBEDDING_DEVICE` | Device for embedding inference | `cuda` |
| `NEO4J_URI` | Neo4j connection URI | `bolt://localhost:7687` |
| `NEO4J_USER` | Neo4j username | `neo4j` |
| `NEO4J_PASSWORD` | Neo4j password | ‚Äî |
| `VECTOR_INDEX_NAME` | Neo4j vector index name | `chunk_embedding_index` |
| `FAISS_URI` | Path to the FAISS index file | ‚Äî |
| `FAISS_METADATA` | Path to the FAISS metadata pickle file | ‚Äî |
| `MILVUS_URI` | Milvus server URI (for indexing step only) | ‚Äî |
| `COLLECTION_NAME` | Milvus collection name | ‚Äî |
| `AVALAI_API_KEY` | AvalAI API key (for entity extraction) | ‚Äî |
| `AVALAI_BASE_URL` | AvalAI base URL | `https://api.avalai.ir/v1` |
| `AVALAI_LLM_MODEL` | Model for entity extraction | `gpt-4o-mini` |
| `AVALAI_REASONING_EFFORT` | Reasoning intensity | `minimal` |

### 5. Build the Vector Database (First Time)

If you are setting up from scratch, run the vector DB builder to embed and index your pharmaceutical chunks:

```bash
python make_vector_db.py
```

> **Note:** This requires a populated `data/rag_chunks_v5.json` file and a running Milvus instance.

### 6. Run the Application

```bash
streamlit run app.py
```

The app will open automatically in your browser at `http://localhost:8501`.

---

## üß© Module Details

### `rag.py` ‚Äî Hybrid Retrieval
Implements a **hybrid BM25 + FAISS retrieval** strategy over pharmaceutical text chunks. At query time, BM25 sparse scores and FAISS dense cosine-similarity scores are each normalized and linearly fused:

```
final_score = Œ± √ó bm25_score + (1 ‚àí Œ±) √ó dense_score
```

The default `Œ± = 0.5` gives equal weight to both methods. Persian text is normalized before BM25 tokenization using character-level substitutions to handle Farsi script variations.

---

### `graphrag.py` ‚Äî Graph RAG Pipeline
The most sophisticated module. It performs:

1. **Persian Text Normalization** using `hazm`
2. **LLM Entity Extraction** ‚Äî calls `entity_extractor.py` to identify entities (drugs, conditions, interactions, etc.) and classify query intents
3. **Entity Linking** ‚Äî matches extracted mentions to nodes in Neo4j via exact lookup ‚Üí fuzzy matching (`rapidfuzz`) ‚Üí semantic embedding similarity
4. **Cypher Query Routing** ‚Äî routes to specialized Cypher queries based on detected intent:

| Intent | Description |
|---|---|
| `INDICATION` | What is this drug used for? |
| `ADVERSE_EFFECT` | What are the side effects? |
| `INTERACTION` | Does this drug interact with X? |
| `CONTRAINDICATION` | When should this drug NOT be used? |
| `CAUTION` | Special warnings and precautions |
| `COMPARISON` | Compare two drugs |
| `GENERAL_INFO` | General drug information |
| `POPULATION_SPECIFIC` | Use in pregnancy, children, elderly, etc. |
| `ASSOCIATION` | Drug-disease associations |
| `CAUSE` | Drug-induced conditions |

---

### `entity_extractor.py` ‚Äî NER & Intent Classification
Uses an OpenAI-compatible LLM API (AvalAI) with **structured JSON output** (strict schema) to extract:
- **Entity types:** `drug`, `condition`, `adverse_effect`, `drug_class`, `interaction_agent`, `population`, `context`, `chemical`
- **Detected language:** `fa`, `en`, `mixed`
- **Query intents:** one or more from the 10 allowed intent classes

Includes up to 3 retry attempts for robust extraction.

---

### `chat.py` ‚Äî Conversation Management
Manages persistent, named chat sessions stored as JSON files. Key features:
- **Automatic summarization** ‚Äî when token count exceeds `MAX_TOKENS` (8192), older messages are summarized by `gemma3:4b` locally via Ollama and stored as `highlights`
- **Multi-session** ‚Äî create, rename, and switch between independent conversations
- **Token tracking** ‚Äî per-message token counting using `tiktoken`

---

### `answer.py` ‚Äî Generation Orchestrator
Ties everything together:
1. Runs `hybrid_retrieve()` from `rag.py`
2. Runs `graph_rag.execute()` from `graphrag.py`
3. Formats a structured prompt with both context blocks and conversation history
4. Streams the response from `gemma3:4b` via Ollama
5. Saves GraphRAG query results as JSON references for traceability

---

### `evaluation.py` ‚Äî Evaluation Pipeline
Runs a batch evaluation over a predefined test set (`test/questions.json`), collecting:
- The generated answer
- The RAG context used
- The GraphRAG context used

Results are saved to `test/results_1.json` and can be fed to an LLM-as-a-Judge scorer.

---

## üí° Key Design Decisions

- **Dual retrieval over single retrieval:** Vector search excels at semantic similarity while the knowledge graph handles structured relational lookups (e.g., "all drugs that interact with warfarin"). Using both maximizes coverage.
- **Local LLM for generation:** `gemma3:4b` via Ollama ensures full data privacy and zero generation API costs, while AvalAI is only used for the lightweight entity-extraction step.
- **Evidence-grounded answers only:** The system prompt strictly prohibits the model from answering outside the retrieved context, ensuring every response is traceable and hallucination-resistant.
- **Persian-first design:** Custom normalization, RTL UI layout, and a warm conversational tone modeled on a pharmacist ‚Äî not a textbook ‚Äî make the chatbot accessible to everyday Persian speakers.

---

## üîÆ Roadmap

- [ ] **Question classifier** ‚Äî a fine-tuned lightweight classifier to pre-categorize user queries before retrieval
- [ ] **Domain fine-tuning** ‚Äî fine-tune `Gemma3-4B` on medical Persian QA datasets
- [ ] **Web search integration** ‚Äî augment knowledge base with live searches on trusted pharmaceutical sources
- [ ] **Richer dataset** ‚Äî expand the knowledge graph and vector store with higher-quality, more comprehensive pharmaceutical data
- [ ] **Milvus ‚Üí FAISS migration in production** ‚Äî consolidate vector storage

---

## üë• Team

| Name | Role |
|---|---|
| Alireza Babazade | Developer |
| Mohammad Mehdi Qanbari | Developer |
| Armita Ghorbani | Developer |
| Sina Mahallati | Developer |

---

## ‚ö†Ô∏è Disclaimer

HooshDaroo is an **experimental research project** intended for educational purposes only. It is **not a substitute for professional medical or pharmaceutical advice**. Always consult a licensed pharmacist or physician before making any medication decisions.

---

## üìÑ License

This project is licensed under the [MIT License](LICENSE).

---

<div align="center">
Made with ‚ù§Ô∏è and ‚òï ‚Äî February 2026
</div>
